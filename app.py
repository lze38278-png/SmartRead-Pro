import streamlit as st
import os
import re
import string
import time
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer


# --- 1. åˆå§‹åŒ– NLP å¼•æ“ ---
@st.cache_resource
def download_nltk_data():
    resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4', 'punkt_tab']
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            try:
                nltk.data.find(f'corpora/{r}')
            except LookupError:
                nltk.download(r)


download_nltk_data()
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))


# --- 2. æ ¸å¿ƒ NLP ç®—æ³• (å·²ä¿®å¤æ•°å­—é—®é¢˜) ---
def process_text(text):
    text = text.lower()
    # å»é™¤æ ‡ç‚¹
    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))
    words = nltk.word_tokenize(text)

    clean_words = []
    for word in words:
        # ğŸŸ¢ ä¿®å¤æ ¸å¿ƒï¼šå¢åŠ  word.isalpha() åˆ¤æ–­
        # å«ä¹‰ï¼šåªæœ‰å½“å•è¯å®Œå…¨ç”±å­—æ¯ç»„æˆæ—¶æ‰ä¿ç•™ (è¿‡æ»¤æ‰ "24", "100%", "2015" ç­‰)
        if word not in stop_words and len(word) > 1 and word.isalpha():
            lemma = lemmatizer.lemmatize(word, pos='v')
            lemma = lemmatizer.lemmatize(lemma, pos='n')
            clean_words.append(lemma)

    return set(clean_words)


# --- 3. æ•°æ®åŠ è½½ ---
@st.cache_data
def load_articles():
    articles = []
    data_folder = 'data'
    if not os.path.exists(data_folder):
        os.makedirs(data_folder)
        return []

    files = os.listdir(data_folder)
    files.sort(reverse=True)

    for filename in files:
        if filename.endswith(".txt"):
            file_path = os.path.join(data_folder, filename)
            try:
                year_match = re.search(r'20\d{2}', filename)
                year = int(year_match.group()) if year_match else 0
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if content.strip():
                        articles.append({
                            "title": filename,
                            "year": year,
                            "content": content
                        })
            except Exception as e:
                pass
    return articles


# --- 4. ç•Œé¢è®¾è®¡ (V0.2.7 é£æ ¼) ---
st.set_page_config(page_title="SmartRead Pro", page_icon="ğŸ“", layout="wide")

# æ ·å¼å¾®è°ƒ
st.markdown("""
<style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}

    /* è§å…‰ç¬”é«˜äº®æ ·å¼ */
    .highlight-marker {
        background-color: rgba(255, 235, 59, 0.6); 
        padding: 0 4px;
        border-radius: 4px;
        font-weight: bold;
        color: #000000;
    }
</style>
""", unsafe_allow_html=True)

# ä¾§è¾¹æ 
with st.sidebar:
    st.title("âš™ï¸ æ§åˆ¶å°")
    st.success("âœ… NLP æ ¸å¿ƒå·²å°±ç»ª")
    st.markdown("---")

    all_articles = load_articles()
    total_count = len(all_articles)

    if total_count > 0:
        years = [a['year'] for a in all_articles if a['year'] > 0]
        min_y, max_y = (min(years), max(years)) if years else (2010, 2025)

        st.subheader("ğŸ“… æ•°æ®é€è§†")
        selected_range = st.slider("å¹´ä»½èŒƒå›´ç­›é€‰", min_y, max_y, (min_y, max_y))

        filtered_articles = [a for a in all_articles if selected_range[0] <= a['year'] <= selected_range[1]]

        col_s1, col_s2 = st.columns(2)
        with col_s1:
            st.metric("æ–‡ç« æ€»æ•°", total_count)
        with col_s2:
            st.metric("å½“å‰é€‰ä¸­", len(filtered_articles))
    else:
        st.error("âš ï¸ æ•°æ®åº“ä¸ºç©º")
        filtered_articles = []
        selected_range = (0, 0)

# ä¸»ç•Œé¢æ ‡é¢˜ (å›å½’ç™½è‰²å¤§å­—)
st.title("ğŸ“ SmartRead è€ƒç ”è‹±è¯­æ™ºèƒ½ä¼´è¯»")
st.caption(f"V0.2.8 ç®—æ³•ä¿®å¤ç‰ˆ | èµ‹èƒ½ä½ çš„æ¯ä¸€åˆ†é’Ÿå¤ä¹  | æ•°æ®æº: {selected_range[0]}-{selected_range[1]}")

# è¾“å…¥åŒº
col1, col2 = st.columns([3, 1])
with col1:
    user_input = st.text_area("åœ¨æ­¤è¾“å…¥ä½ èƒŒçš„å•è¯æˆ–é•¿éš¾å¥ï¼š", height=80,
                              placeholder="è¯•ç€è¾“å…¥: The economic growth rate involves inflation...")
with col2:
    st.write("")
    st.write("")
    search_btn = st.button("ğŸš€ æ·±åº¦åŒ¹é…", type="primary", use_container_width=True)

# --- 5. åŒ¹é…é€»è¾‘ ---
if search_btn:
    if not user_input.strip():
        st.warning("âš ï¸ è¯·å…ˆè¾“å…¥å†…å®¹ï¼")
    elif not filtered_articles:
        st.error("âŒ æ²¡æœ‰æ•°æ®å¯ä¾›æ£€ç´¢ã€‚")
    else:
        # å‡è£…æ€è€ƒçš„è¿›åº¦æ¡
        progress_text = "æ­£åœ¨å»é™¤åœç”¨è¯ã€è¯å½¢è¿˜åŸã€è¿‡æ»¤éæ ¸å¿ƒæ•°å­—..."
        my_bar = st.progress(0, text=progress_text)
        for percent_complete in range(100):
            time.sleep(0.005)
            my_bar.progress(percent_complete + 1, text=progress_text)
        time.sleep(0.2)
        my_bar.empty()

        # æ ¸å¿ƒï¼šå¤„ç†ç”¨æˆ·è¾“å…¥ (æ­¤æ—¶æ•°å­—ä¼šè¢«è¿‡æ»¤æ‰)
        user_lemmas = process_text(user_input)

        with st.expander("ğŸ§  ç‚¹å‡»æŸ¥çœ‹ NLP è¯­ä¹‰åˆ†æå†…æ ¸ (å·²è¿‡æ»¤æ•°å­—å¹²æ‰°)", expanded=True):
            st.write("åŸå§‹è¾“å…¥:", user_input)
            # è¿™é‡Œæ˜¾ç¤ºçš„é›†åˆé‡Œï¼Œç»å¯¹ä¸ä¼šå†æœ‰ '24' äº†
            st.code(f"æ ¸å¿ƒè¯æ ¹æå– (Set): {user_lemmas}", language="python")

        if not user_lemmas:
            st.warning("è¾“å…¥å†…å®¹æ— æ•ˆï¼ˆå¯èƒ½æ˜¯åœç”¨è¯æˆ–çº¯æ•°å­—ï¼‰ï¼Œè¯·è¾“å…¥å®ä¹‰è¯ã€‚")
        else:
            results = []
            for item in filtered_articles:
                article_lemmas = process_text(item['content'])
                common = user_lemmas.intersection(article_lemmas)
                score = len(common)
                if score > 0:
                    item['score'] = score
                    item['matches'] = common
                    results.append(item)

            results.sort(key=lambda x: x['score'], reverse=True)

            if not results:
                st.info("ğŸ¤·â€â™‚ï¸ æœªæ‰¾åˆ°åŒ¹é…æ–‡ç« ã€‚")
            else:
                st.success(f"ğŸ‰ æ£€ç´¢å®Œæˆï¼ä¸ºæ‚¨æ¨è **{len(results)}** ç¯‡é«˜ç›¸å…³çœŸé¢˜")

                for idx, res in enumerate(results[:10]):
                    # å¡ç‰‡å®¹å™¨
                    with st.container(border=True):
                        # å®Œç¾çš„æ ‡é¢˜å¸ƒå±€ï¼šæ’å + å¹´ä»½ + æ ‡é¢˜
                        col_head_1, col_head_2 = st.columns([4, 1])

                        with col_head_1:
                            st.markdown(f"### ğŸ† Top {idx + 1} | [{res['year']}] {res['title']}")
                            st.caption(f"ğŸ¯ å‘½ä¸­å…³é”®è¯: {', '.join(res['matches'])}")

                        with col_head_2:
                            st.metric("åŒ¹é…çƒ­åº¦", res['score'])

                        st.markdown("---")

                        display_content = res['content']
                        for match_word in res['matches']:
                            # æ­£åˆ™å…¨è¯åŒ¹é…é«˜äº®
                            pattern = re.compile(r'\b({})\b'.format(re.escape(match_word)), re.IGNORECASE)
                            display_content = pattern.sub(
                                r'<span class="highlight-marker">\1</span>',
                                display_content
                            )

                        st.markdown(display_content, unsafe_allow_html=True)