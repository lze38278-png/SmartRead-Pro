import streamlit as st
import os
import re
import string
import time
import nltk
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# [V3.6å†…æ ¸] è°·æ­Œç¿»è¯‘
from deep_translator import GoogleTranslator


# --- 1. åˆå§‹åŒ– NLP å¼•æ“ ---
@st.cache_resource
def download_nltk_data():
    resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4', 'punkt_tab']
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            try:
                nltk.data.find(f'corpora/{r}')
            except LookupError:
                nltk.download(r)


download_nltk_data()
lemmatizer = WordNetLemmatizer()
base_stop_words = set(stopwords.words('english'))

academic_stop_words = {
    'text', 'author', 'passage', 'paragraph', 'article',
    'example', 'however', 'although', 'therefore', 'study', 'research'
}
final_stop_words = list(base_stop_words.union(academic_stop_words))


# --- 2. æ ¸å¿ƒè¾…åŠ©å‡½æ•° ---
def process_text_for_display(text):
    text = text.lower()
    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))
    words = nltk.word_tokenize(text)
    clean_words = []
    for word in words:
        if word not in base_stop_words and len(word) > 1 and word.isalpha():
            lemma = lemmatizer.lemmatize(word, pos='v')
            lemma = lemmatizer.lemmatize(lemma, pos='n')
            clean_words.append(lemma)
    return set(clean_words)


def get_article_category_by_name(filename):
    name_lower = filename.lower()
    if "eng1" in name_lower or "è‹±è¯­ä¸€" in name_lower:
        return "è‹±è¯­ä¸€"
    elif "eng2" in name_lower or "è‹±è¯­äºŒ" in name_lower:
        return "è‹±è¯­äºŒ"
    elif "cet4" in name_lower or "å››çº§" in name_lower:
        return "å››çº§"
    elif "cet6" in name_lower or "å…­çº§" in name_lower:
        return "å…­çº§"
    else:
        return "å…¶ä»–"


# [V3.6å†…æ ¸] æ™ºèƒ½åˆ†ç‰‡ç¿»è¯‘
@st.cache_data(show_spinner=False)
def translate_text(text):
    try:
        sentences = nltk.sent_tokenize(text)
        chunks = []
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk) + len(sentence) < 1000:
                current_chunk += sentence + " "
            else:
                chunks.append(current_chunk)
                current_chunk = sentence + " "
        if current_chunk:
            chunks.append(current_chunk)

        full_translation = ""
        translator = GoogleTranslator(source='auto', target='zh-CN')

        for chunk in chunks:
            if chunk.strip():
                time.sleep(0.2)
                trans = translator.translate(chunk)
                if trans:
                    full_translation += trans + " "
        return full_translation
    except Exception as e:
        return f"ç¿»è¯‘å¼‚å¸¸: {str(e)}"


# ğŸŸ¢ [V3.8 æ–°å¢] SmartBridge è§£æå™¨
def parse_vocabulary_paste(text):
    """
    æ™ºèƒ½è§£æå‰ªè´´æ¿å†…å®¹
    æ”¯æŒæ ¼å¼ï¼š
    1. apple n. è‹¹æœ
    2. banana [éŸ³æ ‡] é¦™è•‰
    3. çº¯å•è¯åˆ—è¡¨
    """
    vocab_set = set()
    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        if not line: continue

        # ç­–ç•¥Aï¼šå°è¯•åŒ¹é…è¡Œé¦–çš„çº¯è‹±æ–‡å•è¯
        # æ’é™¤åƒ 'a', 'I' è¿™ç§è¿‡çŸ­çš„è¯ï¼Œé™¤éæ˜ç¡®å°±æ˜¯ä¸ªå•è¯è¡Œ
        match = re.match(r'^[a-zA-Z\-\']{2,}', line)
        if match:
            word = match.group()
            # å†æ¬¡æ¸…æ´—ï¼Œå»æ‰éå­—æ¯å­—ç¬¦
            clean_word = re.sub(r'[^a-zA-Z\-]', '', word).lower()
            if clean_word not in base_stop_words:
                vocab_set.add(clean_word)

    return list(vocab_set)


# --- 3. æ•°æ®åŠ è½½ ---
@st.cache_data
def load_articles():
    articles = []
    data_folder = 'data'
    if not os.path.exists(data_folder):
        os.makedirs(data_folder)
        return []

    for root, dirs, files in os.walk(data_folder):
        for filename in files:
            if filename.endswith(".txt") or filename.endswith(".json"):  # é¢„ç•™jsonæ¥å£
                file_path = os.path.join(root, filename)
                try:
                    year_match = re.search(r'20\d{2}', filename)
                    year = int(year_match.group()) if year_match else 0
                    folder_name = os.path.basename(root)

                    category = get_article_category_by_name(filename) if folder_name == 'data' else folder_name

                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    if content.strip():
                        # é¢„å¤„ç†å¼•ç†ï¼ŒåŠ é€Ÿåç»­åŒ¹é…
                        lemmas = process_text_for_display(content)
                        articles.append({
                            "title": filename,
                            "year": year,
                            "category": category,
                            "content": content,
                            "lemmas": lemmas  # ç¼“å­˜å¼•ç†é›†åˆ
                        })
                except Exception:
                    pass
    articles.sort(key=lambda x: x['year'], reverse=True)
    return articles


# --- 4. ç•Œé¢è®¾è®¡ ---
st.set_page_config(page_title="SmartRead V3.8", page_icon="ğŸ“", layout="wide")

st.markdown("""
<style>
    div.stButton > button {
        width: 100%;
        min-height: 50px;
        font-size: 18px !important;
        border-radius: 10px;
    }
    .stTextArea textarea {
        font-size: 16px !important;
    }
    .highlight-marker {
        background-color: #ffeb3b;
        padding: 0 4px;
        border-radius: 4px;
        font-weight: bold;
        color: #000;
    }
    .vocab-badge {
        background-color: #e3f2fd;
        color: #1565c0;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.9em;
        margin-right: 5px;
        display: inline-block;
        margin-bottom: 5px;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“ SmartRead è€ƒç ”è‹±è¯­æ™ºèƒ½ä¼´è¯»")

# æ•°æ®åŠ è½½
all_articles = load_articles()
total_count = len(all_articles)

# ç­›é€‰å™¨ (Expander)
if total_count > 0:
    years = [a['year'] for a in all_articles if a['year'] > 0]
    min_y, max_y = (min(years), max(years)) if years else (2010, 2025)
    available_categories = sorted(list(set([a['category'] for a in all_articles])))

    with st.expander("âš™ï¸ é¢˜åº“ç­›é€‰è®¾ç½® (ç‚¹å‡»å±•å¼€)", expanded=False):
        selected_cats = st.multiselect("ğŸ“š è¯•å·ç±»å‹:", available_categories, default=available_categories)
        selected_range = st.slider("ğŸ“… å¹´ä»½èŒƒå›´", min_y, max_y, (min_y, max_y))

        filtered_articles = [
            a for a in all_articles
            if (selected_range[0] <= a['year'] <= selected_range[1]) and (a['category'] in selected_cats)
        ]
        st.caption(f"å½“å‰æ¿€æ´»æ–‡ç« åº“: {len(filtered_articles)} ç¯‡")
else:
    filtered_articles = []
    st.error("âš ï¸ æ•°æ®åº“ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ data æ–‡ä»¶å¤¹")

# ==========================================
# ğŸŸ¢ V3.8 æ ¸å¿ƒå‡çº§ï¼šåŒæ ‡ç­¾é¡µæ¶æ„
# ==========================================
tab1, tab2 = st.tabs(["ğŸ” æŸ¥è¯ä¸ç ”è¯»", "ğŸ“¥ å¯¼å…¥ç”Ÿè¯æœ¬ (SmartBridge)"])

# --- TAB 1: åŸæœ‰çš„æŸ¥è¯åŠŸèƒ½ ---
with tab1:
    col1, col2 = st.columns([3, 1])
    with col1:
        user_input = st.text_area("è¾“å…¥å•è¯æˆ–é•¿éš¾å¥ï¼š", height=100, placeholder="ä¾‹å¦‚: artificial intelligence...",
                                  key="search_box")
    with col2:
        st.write("")
        st.write("")
        search_btn = st.button("ğŸš€ å‘é‡æ£€ç´¢", type="primary", key="btn_search")

    if search_btn and user_input.strip() and filtered_articles:
        # TF-IDF é€»è¾‘ (ä¿æŒä¸å˜)
        progress_text = "SmartRead æ­£åœ¨æ£€ç´¢..."
        my_bar = st.progress(0, text=progress_text)
        time.sleep(0.1)
        my_bar.empty()

        corpus = [item['content'] for item in filtered_articles]
        corpus.append(user_input)

        try:
            tfidf_vectorizer = TfidfVectorizer(stop_words=final_stop_words, max_df=0.6, min_df=1)
            tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)
            similarity_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()

            results = []
            user_lemmas = process_text_for_display(user_input)

            for idx, score in enumerate(similarity_scores):
                if score > 0.05:
                    item = filtered_articles[idx]
                    item['score'] = score
                    item['matches'] = user_lemmas.intersection(item['lemmas'])
                    results.append(item)

            results.sort(key=lambda x: x['score'], reverse=True)

            if results:
                st.success(f"ğŸ‰ æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³çœŸé¢˜")
                for res in results[:5]:
                    with st.container(border=True):
                        st.markdown(f"### ã€{res['category']}ã€‘{res['title']}")
                        st.caption(f"ç›¸å…³åº¦: {round(res['score'] * 100, 1)}% | å‘½ä¸­: {', '.join(res['matches'])}")

                        display_content = res['content']
                        for match_word in res['matches']:
                            pattern = re.compile(r'\b({})\b'.format(re.escape(match_word)), re.IGNORECASE)
                            display_content = pattern.sub(r'<span class="highlight-marker">\1</span>', display_content)
                        st.markdown(display_content, unsafe_allow_html=True)

                        with st.expander("ğŸ‡¨ğŸ‡³ æŸ¥çœ‹ç¿»è¯‘"):
                            st.write(translate_text(res['content']))
            else:
                st.info("æœªæ‰¾åˆ°åŒ¹é…æ–‡ç« ")
        except ValueError:
            st.warning("è¾“å…¥è¯æ±‡è¿‡äºç”Ÿåƒ»æˆ–è¢«åœç”¨è¯è¿‡æ»¤")

# --- TAB 2: SmartBridge ç”Ÿè¯å¯¼å…¥åŠŸèƒ½ ---
with tab2:
    st.markdown("#### ğŸ”— SmartBridgeï¼šä»èƒŒå•è¯ App ä¸€é”®å¯¼å…¥")
    st.info(
        "ğŸ’¡ æ“ä½œæŒ‡å—ï¼šåœ¨å¢¨å¢¨/ä¸èƒŒå•è¯ä¸­ç‚¹å‡»ã€å¤åˆ¶ä»Šæ—¥å•è¯ã€‘ï¼Œç„¶åç›´æ¥ç²˜è´´åœ¨ä¸‹æ–¹ã€‚ç³»ç»Ÿå°†ä¸ºä½ æ¨èåŒ…å«è¿™äº›ç”Ÿè¯æœ€å¤šçš„çœŸé¢˜æ–‡ç« ã€‚")

    paste_text = st.text_area("è¯·ç²˜è´´ç”Ÿè¯åˆ—è¡¨ï¼š", height=150, placeholder="ä¾‹å¦‚ï¼š\nabandon v. æ”¾å¼ƒ\nability n. èƒ½åŠ›\n...")

    if st.button("ğŸ“Š ç”Ÿæˆé˜…è¯»æ¨èè®¡åˆ’", type="primary", key="btn_bridge"):
        if not paste_text.strip():
            st.warning("âš ï¸ è¯·å…ˆç²˜è´´å†…å®¹ï¼")
        elif not filtered_articles:
            st.error("âŒ æ–‡ç« åº“ä¸ºç©º")
        else:
            # 1. è§£æç²˜è´´æ¿
            vocab_list = parse_vocabulary_paste(paste_text)

            if not vocab_list:
                st.error("âŒ æœªè¯†åˆ«åˆ°æœ‰æ•ˆå•è¯ï¼Œè¯·æ£€æŸ¥å¤åˆ¶æ ¼å¼ã€‚")
            else:
                st.success(f"âœ… æˆåŠŸè¯†åˆ« {len(vocab_list)} ä¸ªç”Ÿè¯")
                # å±•ç¤ºè¯†åˆ«åˆ°çš„è¯æ³¡æ³¡
                vocab_html = "".join([f'<span class="vocab-badge">{w}</span>' for w in vocab_list])
                st.markdown(vocab_html, unsafe_allow_html=True)

                st.divider()
                st.markdown("### ğŸ† ä»Šæ—¥é˜…è¯»æ¨è (æœ€å¤§è¦†ç›–åŒ¹é…)")

                # 2. è¿è¡Œæœ€å¤§è¦†ç›–ç®—æ³• (Maximum Coverage)
                # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç»Ÿè®¡å­¦é€»è¾‘ï¼šè®¡ç®—æ–‡ç« ä¸­åŒ…å«äº†å¤šå°‘ä¸ªç”¨æˆ·ç”Ÿè¯
                recommendations = []
                user_vocab_set = set(vocab_list)

                for item in filtered_articles:
                    # è®¡ç®—äº¤é›†
                    intersection = user_vocab_set.intersection(item['lemmas'])
                    if intersection:
                        recommendations.append({
                            "article": item,
                            "hits": len(intersection),
                            "hit_words": intersection,
                            "coverage": len(intersection) / len(user_vocab_set)
                        })

                # æŒ‰å‘½ä¸­è¯æ•°é™åºæ’åˆ—
                recommendations.sort(key=lambda x: x['hits'], reverse=True)

                if not recommendations:
                    st.warning("ğŸ¤” ä½ çš„ç”Ÿè¯å¤ªç”Ÿåƒ»äº†ï¼Œå½“å‰çœŸé¢˜åº“é‡Œå±…ç„¶ä¸€ç¯‡éƒ½æ²¡ç¢°ä¸Š...")
                else:
                    for idx, rec in enumerate(recommendations[:5]):
                        art = rec['article']
                        hits = rec['hits']
                        hit_words = rec['hit_words']

                        with st.container(border=True):
                            c1, c2 = st.columns([4, 1])
                            with c1:
                                st.markdown(f"#### Rank {idx + 1} | {art['title']}")
                                st.write(f"åŒ…å«ä½ ç”Ÿè¯æœ¬ä¸­çš„ **{hits}** ä¸ªè¯")
                            with c2:
                                st.metric("è¦†ç›–ç‡", f"{round(rec['coverage'] * 100, 1)}%")

                            # é«˜äº®æ˜¾ç¤ºå‘½ä¸­çš„ç”Ÿè¯
                            display_text = art['content']
                            for hw in hit_words:
                                pattern = re.compile(r'\b({})\b'.format(re.escape(hw)), re.IGNORECASE)
                                display_text = pattern.sub(r'<span class="highlight-marker">\1</span>', display_text)

                            with st.expander("ğŸ“„ é˜…è¯»æ–‡ç«  (å·²é«˜äº®ç”Ÿè¯)"):
                                st.markdown(display_text, unsafe_allow_html=True)
                                st.write("---")
                                st.caption(f"ğŸ¯ å‘½ä¸­çš„ç”Ÿè¯: {', '.join(hit_words)}")