import streamlit as st
import os
import re
import string
import time
import nltk
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# --- 1. åˆå§‹åŒ– NLP å¼•æ“ ---
@st.cache_resource
def download_nltk_data():
    resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4', 'punkt_tab']
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}')
        except LookupError:
            try:
                nltk.data.find(f'corpora/{r}')
            except LookupError:
                nltk.download(r)


download_nltk_data()
lemmatizer = WordNetLemmatizer()
base_stop_words = set(stopwords.words('english'))

# å­¦æœ¯åœç”¨è¯è¡¨
academic_stop_words = {
    'text', 'author', 'passage', 'paragraph', 'article',
    'example', 'however', 'although', 'therefore', 'study', 'research'
}
final_stop_words = list(base_stop_words.union(academic_stop_words))


# --- 2. æ ¸å¿ƒè¾…åŠ©å‡½æ•° ---

def process_text_for_display(text):
    """ç”¨äºå‰ç«¯é«˜äº®å±•ç¤ºçš„å¤„ç†"""
    text = text.lower()
    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))
    words = nltk.word_tokenize(text)
    clean_words = []
    for word in words:
        if word not in base_stop_words and len(word) > 1 and word.isalpha():
            lemma = lemmatizer.lemmatize(word, pos='v')
            lemma = lemmatizer.lemmatize(lemma, pos='n')
            clean_words.append(lemma)
    return set(clean_words)


def get_article_category_by_name(filename):
    """åŸºäºæ–‡ä»¶åçš„å¤‡ç”¨åˆ†ç±»é€»è¾‘"""
    name_lower = filename.lower()
    if "eng1" in name_lower or "è‹±è¯­ä¸€" in name_lower:
        return "è‹±è¯­ä¸€"
    elif "eng2" in name_lower or "è‹±è¯­äºŒ" in name_lower:
        return "è‹±è¯­äºŒ"
    elif "cet4" in name_lower or "å››çº§" in name_lower:
        return "å››çº§"
    elif "cet6" in name_lower or "å…­çº§" in name_lower:
        return "å…­çº§"
    else:
        return "å…¶ä»–"


# --- 3. æ•°æ®åŠ è½½ (V3.5 æ ¸å¿ƒå‡çº§ï¼šé€’å½’è¯»å–) ---
@st.cache_data
def load_articles():
    articles = []
    data_folder = 'data'

    # å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º
    if not os.path.exists(data_folder):
        os.makedirs(data_folder)
        return []

    # ğŸŸ¢ [å‡çº§ç‚¹] ä½¿ç”¨ os.walk éå†æ‰€æœ‰å­æ–‡ä»¶å¤¹
    for root, dirs, files in os.walk(data_folder):
        for filename in files:
            if filename.endswith(".txt"):
                file_path = os.path.join(root, filename)
                try:
                    # å°è¯•ä»æ–‡ä»¶åæå–å¹´ä»½
                    year_match = re.search(r'20\d{2}', filename)
                    year = int(year_match.group()) if year_match else 0

                    # ğŸŸ¢ [å‡çº§ç‚¹] ä¼˜å…ˆç”¨æ–‡ä»¶å¤¹åå­—åšåˆ†ç±»
                    # root æ˜¯å½“å‰æ–‡ä»¶çš„è·¯å¾„ï¼Œos.path.basename(root) å°±æ˜¯æ–‡ä»¶å¤¹åï¼ˆå¦‚ "å…­çº§"ï¼‰
                    folder_name = os.path.basename(root)

                    # å¦‚æœæ–‡ä»¶ç›´æ¥åœ¨ data æ ¹ç›®å½•ä¸‹ï¼Œåˆ™å°è¯•ç”¨æ–‡ä»¶ååˆ¤æ–­
                    if folder_name == 'data':
                        category = get_article_category_by_name(filename)
                    else:
                        category = folder_name

                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    if content.strip():
                        articles.append({
                            "title": filename,
                            "year": year,
                            "category": category,
                            "content": content
                        })
                except Exception as e:
                    # é‡åˆ°ç¼–ç é”™è¯¯æˆ–å…¶ä»–é—®é¢˜è·³è¿‡
                    print(f"Skipping {filename}: {e}")
                    pass

    # æŒ‰å¹´ä»½å€’åºæ’åˆ—
    articles.sort(key=lambda x: x['year'], reverse=True)
    return articles


# --- 4. ç•Œé¢è®¾è®¡ ---
st.set_page_config(page_title="SmartRead Pro V3.5", page_icon="ğŸ“", layout="wide")

st.markdown("""
<style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .highlight-marker {
        background-color: rgba(255, 235, 59, 0.6);
        padding: 0 4px;
        border-radius: 4px;
        font-weight: bold;
        color: #000000;
    }
</style>
""", unsafe_allow_html=True)

with st.sidebar:
    st.title("âš™ï¸ æ™ºç®—ä¸­å¿ƒ")
    st.success("âœ… TF-IDF ç®—æ³•å·²è°ƒä¼˜")
    st.info("âš¡ Max-DF é™å™ª | é€’å½’è¯»å–")
    st.markdown("---")

    # 1. åŠ è½½æ‰€æœ‰æ•°æ®
    all_articles = load_articles()
    total_count = len(all_articles)

    if total_count > 0:
        # è·å–æ‰€æœ‰å¹´ä»½
        years = [a['year'] for a in all_articles if a['year'] > 0]
        # é˜²æ­¢åªæœ‰0å¹´å¯¼è‡´æŠ¥é”™
        if years:
            min_y, max_y = (min(years), max(years))
        else:
            min_y, max_y = (2010, 2025)

        st.subheader("ğŸ“… è¯­æ–™åº“èŒƒå›´")
        selected_range = st.slider("å¹´ä»½ç­›é€‰", min_y, max_y, (min_y, max_y))

        # è¯•å·ç±»å‹å¤šé€‰æ¡†
        available_categories = sorted(list(set([a['category'] for a in all_articles])))

        # é»˜è®¤å…¨éƒ¨é€‰ä¸­
        selected_cats = st.multiselect(
            "ğŸ“š è¯•å·ç±»å‹ (å¯å¤šé€‰)",
            options=available_categories,
            default=available_categories
        )

        # æ ¸å¿ƒç­›é€‰é€»è¾‘
        filtered_articles = [
            a for a in all_articles
            if (selected_range[0] <= a['year'] <= selected_range[1]) and (a['category'] in selected_cats)
        ]

        col_s1, col_s2 = st.columns(2)
        with col_s1:
            st.metric("æ–‡ç« æ€»é‡", total_count)
        with col_s2:
            st.metric("æ¿€æ´»æ–‡ç« ", len(filtered_articles))
    else:
        # ğŸŸ¢ [ä¿®å¤ç‚¹] ä¹‹å‰è¿™é‡Œæ²¡å®šä¹‰ selected_catsï¼Œå¯¼è‡´åç»­æŠ¥é”™
        st.error("âš ï¸ æ•°æ®åº“ä¸ºç©º")
        st.caption("è¯·åœ¨ data æ–‡ä»¶å¤¹ä¸‹æ”¾å…¥ txt çœŸé¢˜æ–‡ä»¶")
        filtered_articles = []
        selected_range = (0, 0)
        selected_cats = []

st.title("ğŸ“ SmartRead è€ƒç ”è‹±è¯­æ™ºèƒ½ä¼´è¯»")
st.caption(
    f"V3.5 é€’å½’è¯»å–åŠ å¼ºç‰ˆ | æ•°æ®æº: {selected_range[0]}-{selected_range[1]} | ç±»å‹: {', '.join(selected_cats) if selected_cats else 'æ— '}")

col1, col2 = st.columns([3, 1])
with col1:
    user_input = st.text_area("åœ¨æ­¤è¾“å…¥å•è¯æˆ–é•¿éš¾å¥ï¼š", height=80,
                              placeholder="ä¾‹å¦‚: First generation college students struggle with social class disadvantages...")
with col2:
    st.write("")
    st.write("")
    search_btn = st.button("ğŸš€ å‘é‡æ£€ç´¢", type="primary", use_container_width=True)

# --- 5. æ ¸å¿ƒï¼šTF-IDF åŒ¹é…ç®—æ³• ---
if search_btn:
    if not user_input.strip():
        st.warning("âš ï¸ è¯·è¾“å…¥å†…å®¹ï¼")
    elif not filtered_articles:
        st.error("âŒ å½“å‰ç­›é€‰æ¡ä»¶ä¸‹æ— æ–‡ç« ï¼Œè¯·æ£€æŸ¥å·¦ä¾§ç­›é€‰æ ã€‚")
    else:
        progress_text = "æ­£åœ¨æ‰§è¡Œ Max-DF é™å™ª | æ„å»ºåŠ æƒçŸ©é˜µ..."
        my_bar = st.progress(0, text=progress_text)
        for percent_complete in range(100):
            time.sleep(0.005)
            my_bar.progress(percent_complete + 1, text=progress_text)
        time.sleep(0.2)
        my_bar.empty()

        corpus = [item['content'] for item in filtered_articles]
        corpus.append(user_input)

        tfidf_vectorizer = TfidfVectorizer(
            stop_words=final_stop_words,
            max_df=0.6,
            min_df=1
        )

        try:
            tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)
            user_vector = tfidf_matrix[-1]
            document_vectors = tfidf_matrix[:-1]
            similarity_scores = cosine_similarity(user_vector, document_vectors).flatten()

            results = []
            user_lemmas_for_highlight = process_text_for_display(user_input)

            for idx, score in enumerate(similarity_scores):
                if score > 0.05:
                    item = filtered_articles[idx]
                    item['score'] = score
                    article_lemmas = process_text_for_display(item['content'])
                    item['matches'] = user_lemmas_for_highlight.intersection(article_lemmas)
                    results.append(item)

            results.sort(key=lambda x: x['score'], reverse=True)

            if not results:
                st.info("ğŸ¤·â€â™‚ï¸ æœªæ‰¾åˆ°è¯­ä¹‰ç›¸å…³çš„æ–‡ç« ã€‚")
            else:
                st.success(f"ğŸ‰ æ£€ç´¢å®Œæˆï¼ä¸ºæ‚¨æ¨è **{len(results)}** ç¯‡é«˜ç›¸å…³çœŸé¢˜")

                for idx, res in enumerate(results[:10]):
                    with st.container(border=True):
                        col_head_1, col_head_2 = st.columns([4, 1])
                        score_percent = round(res['score'] * 100, 1)

                        with col_head_1:
                            category_badge = f"ã€{res['category']}ã€‘"
                            st.markdown(f"### ğŸ† Top {idx + 1} | {category_badge} [{res['year']}] {res['title']}")
                            match_str = ', '.join(res['matches']) if res['matches'] else "è¯­ä¹‰é«˜åº¦ç›¸å…³"
                            st.caption(f"ğŸ¯ å‘½ä¸­å…³é”®è¯: {match_str}")

                        with col_head_2:
                            st.metric("ç›¸å…³åº¦", f"{score_percent}%")

                        st.markdown("---")

                        display_content = res['content']
                        for match_word in res['matches']:
                            pattern = re.compile(r'\b({})\b'.format(re.escape(match_word)), re.IGNORECASE)
                            display_content = pattern.sub(
                                r'<span class="highlight-marker">\1</span>',
                                display_content
                            )
                        st.markdown(display_content, unsafe_allow_html=True)

        except ValueError:
            st.warning("âš ï¸ æ— æ³•æ„å»ºå‘é‡ç©ºé—´ï¼Œè¯·å°è¯•è¾“å…¥æ›´å…·ä½“çš„å®ä¹‰è¯ã€‚")